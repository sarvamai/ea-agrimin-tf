[
  {
    "name": "K8S Pod Not Running",
    "is_paused": false,
    "severity": "critical",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-001-pod-not-running",
    "expression": "sum by (namespace, pod) (kube_pod_status_phase{phase=~\"Pending|Failed|Unknown\"}) > 0",
    "threshold": 0,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\\n*Pod:* {{ with $values.A }}{{ .Labels.pod }}{{ end }}\\n\\nCRITICAL: Pod is not in Running state. Check pod status: `kubectl describe pod {{ with $values.A }}{{ .Labels.pod }}{{ end }} -n {{ with $values.A }}{{ .Labels.namespace }}{{ end }}`",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nPod is now running normally."
    },
    "for": "2m"
  },
  {
    "name": "K8S Deployment Replica Mismatch",
    "is_paused": false,
    "severity": "critical",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-002-deployment-replica-mismatch",
    "expression": "(kube_deployment_spec_replicas - kube_deployment_status_replicas_available) > 0",
    "threshold": 0,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\\n*Deployment:* {{ with $values.A }}{{ .Labels.deployment }}{{ end }}\\n\\nCRITICAL: Deployment has fewer available replicas than desired. Missing {{ with $values.A }}{{ .Value }}{{ end }} replica(s).",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nDeployment replicas are now fully available."
    },
    "for": "3m"
  },
  {
    "name": "K8S StatefulSet Replica Mismatch",
    "is_paused": false,
    "severity": "critical",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-003-statefulset-replica-mismatch",
    "expression": "(kube_statefulset_replicas - kube_statefulset_status_replicas_ready) > 0",
    "threshold": 0,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\\n*StatefulSet:* {{ with $values.A }}{{ .Labels.statefulset }}{{ end }}\\n\\nCRITICAL: StatefulSet has fewer ready pods than desired. DO NOT manually delete StatefulSet pods.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nStatefulSet replicas are now fully ready."
    },
    "for": "5m"
  },
  {
    "name": "K8S DaemonSet Not Ready",
    "is_paused": false,
    "severity": "critical",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-004-daemonset-not-ready-on-all-nodes",
    "expression": "(kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_number_ready) > 0",
    "threshold": 0,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\\n*DaemonSet:* {{ with $values.A }}{{ .Labels.daemonset }}{{ end }}\\n\\nCRITICAL: DaemonSet pods are not ready on all schedulable nodes. Check tolerations and node taints.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nDaemonSet is now running on all scheduled nodes."
    },
    "for": "5m"
  },
  {
    "name": "K8S Job Failure Rate High",
    "is_paused": false,
    "severity": "critical",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-005-job-failure-rate-high",
    "expression": "sum by (namespace, job_name) (increase(kube_job_status_failed{job_name!=\"\"}[5m])) > 0",
    "threshold": 0,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\\n*Job:* {{ with $values.A }}{{ .Labels.job_name }}{{ end }}\\n\\nCRITICAL: Job failure detected. Review job logs for errors.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nJob failures have stopped."
    },
    "for": "5m"
  },
  {
    "name": "K8S Node CPU Critical",
    "is_paused": false,
    "severity": "critical",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-006-node-cpu-critical",
    "expression": "(1 - avg by (instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m]))) > 0.95",
    "threshold": 0.95,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Node:* {{ with $values.A }}{{ .Labels.instance }}{{ end }}\\n\\nCRITICAL: Node CPU usage > 95%. Risk of pod scheduling issues and performance degradation.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nNode CPU usage has returned to normal levels."
    },
    "for": "5m"
  },
  {
    "name": "K8S Node Memory Critical",
    "is_paused": false,
    "severity": "critical",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-007-node-memory-critical",
    "expression": "(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) > 0.95",
    "threshold": 0.95,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Node:* {{ with $values.A }}{{ .Labels.instance }}{{ end }}\\n\\nCRITICAL: Node memory usage > 95%. Risk of OOM kills and pod evictions.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nNode memory usage has returned to normal levels."
    },
    "for": "5m"
  },
  {
    "name": "K8S Node Disk Space Critical",
    "is_paused": false,
    "severity": "critical",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-008-node-disk-space-critical",
    "expression": "(1 - (node_filesystem_avail_bytes{fstype!~\"tmpfs|overlay\", mountpoint=\"/\"} / node_filesystem_size_bytes{fstype!~\"tmpfs|overlay\", mountpoint=\"/\"})) > 0.90",
    "threshold": 0.90,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Node:* {{ with $values.A }}{{ .Labels.instance }}{{ end }}\\n*Mountpoint:* {{ with $values.A }}{{ .Labels.mountpoint }}{{ end }}\\n\\nCRITICAL: Node disk usage > 90%. Risk of pod failures and inability to write logs.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nNode disk usage has returned to normal levels."
    },
    "for": "2m"
  },
  {
    "name": "K8S Pod CPU Throttling",
    "is_paused": false,
    "severity": "critical",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-009-pod-cpu-throttling",
    "expression": "sum by (namespace, pod) (rate(container_cpu_cfs_throttled_seconds_total{container!=\"\", container!=\"POD\"}[5m])) > 0.1",
    "threshold": 0.1,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\\n*Pod:* {{ with $values.A }}{{ .Labels.pod }}{{ end }}\\n\\nCRITICAL: Pod is experiencing CPU throttling (>10% of time). Consider increasing CPU limits.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nPod CPU throttling has stopped."
    },
    "for": "5m"
  },
  {
    "name": "K8S Pod Memory Near Limit",
    "is_paused": false,
    "severity": "critical",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-010-pod-memory-approaching-limit",
    "expression": "sum by (namespace, pod) (container_memory_working_set_bytes{container!=\"\", container!=\"POD\", pod!~\"anetd-.*\"}) \n/ \nsum by (namespace, pod) (kube_pod_container_resource_limits{resource=\"memory\", pod!~\"anetd-.*\"})",
    "threshold": 0.95,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\\n*Pod:* {{ with $values.A }}{{ .Labels.pod }}{{ end }}\\n\\nCRITICAL: Pod memory usage > 95% of limit. Risk of OOM kill imminent.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nPod memory usage has returned to safe levels."
    },
    "for": "3m"
  },
  {
    "name": "K8S High Network Error Rate - Node",
    "is_paused": false,
    "severity": "critical",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-011-high-network-error-rate",
    "expression": "sum by (instance) (rate(node_network_receive_errs_total[5m]) + rate(node_network_transmit_errs_total[5m])) > 100",
    "threshold": 100,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Node:* {{ with $values.A }}{{ .Labels.instance }}{{ end }}\\n\\nCRITICAL: High network error rate on node (>100/sec). Check network interface and policies.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nNode network errors have returned to normal."
    },
    "for": "5m"
  },
  {
    "name": "K8S High Network Error Rate - Pod",
    "is_paused": false,
    "severity": "critical",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-011-high-network-error-rate",
    "expression": "sum by (namespace, pod) (rate(container_network_receive_errors_total[5m]) + rate(container_network_transmit_errors_total[5m])) > 10",
    "threshold": 10,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\\n*Pod:* {{ with $values.A }}{{ .Labels.pod }}{{ end }}\\n\\nCRITICAL: High network error rate on pod (>10/sec). Check network policies and DNS.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nPod network errors have returned to normal."
    },
    "for": "5m"
  },
  {
    "name": "K8S Deployment Replica Mismatch Warning",
    "is_paused": false,
    "severity": "warning",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-012-deployment-replica-mismatch-warning",
    "expression": "((kube_deployment_spec_replicas - kube_deployment_status_replicas_available) / kube_deployment_spec_replicas) > 0.1",
    "threshold": 0.1,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\\n*Deployment:* {{ with $values.A }}{{ .Labels.deployment }}{{ end }}\\n\\nWARNING: >10% of deployment replicas unavailable. Monitor trend before it becomes critical.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nDeployment replica availability has improved."
    },
    "for": "5m"
  },
  {
    "name": "K8S Node CPU High",
    "is_paused": false,
    "severity": "warning",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-013-node-cpu-high",
    "expression": "(1 - avg by (instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m]))) > 0.80",
    "threshold": 0.80,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Node:* {{ with $values.A }}{{ .Labels.instance }}{{ end }}\\n\\nWARNING: Node CPU usage > 80%. Consider scaling or optimizing workloads.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nNode CPU usage has returned to normal levels."
    },
    "for": "10m"
  },
  {
    "name": "K8S Node Memory High",
    "is_paused": false,
    "severity": "warning",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-014-node-memory-high",
    "expression": "(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) > 0.80",
    "threshold": 0.80,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Node:* {{ with $values.A }}{{ .Labels.instance }}{{ end }}\\n\\nWARNING: Node memory usage > 80%. Monitor for memory leaks or consider scaling.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nNode memory usage has returned to normal levels."
    },
    "for": "10m"
  },
  {
    "name": "K8S Node Disk Space Warning",
    "is_paused": false,
    "severity": "warning",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-015-node-disk-space-warning",
    "expression": "(1 - (node_filesystem_avail_bytes{fstype!~\"tmpfs|overlay\", mountpoint=\"/\"} / node_filesystem_size_bytes{fstype!~\"tmpfs|overlay\", mountpoint=\"/\"})) > 0.80",
    "threshold": 0.80,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Node:* {{ with $values.A }}{{ .Labels.instance }}{{ end }}\\n*Mountpoint:* {{ with $values.A }}{{ .Labels.mountpoint }}{{ end }}\\n\\nWARNING: Node disk usage > 80%. Review log rotation and image garbage collection.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nNode disk usage has returned to normal levels."
    },
    "for": "10m"
  },
  {
    "name": "K8S Pod CPU High",
    "is_paused": false,
    "severity": "warning",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-016-pod-cpu-high",
    "expression": "sum by (namespace, pod) (rate(container_cpu_usage_seconds_total{container!=\"\", container!=\"POD\"}[5m])) / sum by (namespace, pod) (kube_pod_container_resource_limits{resource=\"cpu\"}) > 0.80",
    "threshold": 0.80,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\\n*Pod:* {{ with $values.A }}{{ .Labels.pod }}{{ end }}\\n\\nWARNING: Pod CPU usage > 80% of limit. Consider increasing CPU limit or optimizing.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nPod CPU usage has returned to normal levels."
    },
    "for": "10m"
  },
  {
    "name": "K8S Pod Memory High",
    "is_paused": false,
    "severity": "warning",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-017-pod-memory-high",
    "expression": "sum by (namespace, pod) (container_memory_working_set_bytes{container!=\"\", container!=\"POD\", pod!~\"anetd-.*\"}) / sum by (namespace, pod) (kube_pod_container_resource_limits{resource=\"memory\", pod!~\"anetd-.*\"})",
    "threshold": 0.80,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\\n*Pod:* {{ with $values.A }}{{ .Labels.pod }}{{ end }}\\n\\nWARNING: Pod memory usage > 80% of limit. Check for memory leaks.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nPod memory usage has returned to normal levels."
    },
    "for": "10m"
  },
  {
    "name": "K8S Pod Filesystem Usage High",
    "is_paused": false,
    "severity": "warning",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-018-pod-filesystem-usage-high",
    "expression": "sum by (namespace, pod) (container_fs_usage_bytes{container!=\"\", container!=\"POD\"}) / sum by (namespace, pod) (container_fs_limit_bytes{container!=\"\", container!=\"POD\"}) > 0.80",
    "threshold": 0.80,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\\n*Pod:* {{ with $values.A }}{{ .Labels.pod }}{{ end }}\\n\\nWARNING: Pod filesystem usage > 80%. Review application logging and file cleanup.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nPod filesystem usage has returned to normal levels."
    },
    "for": "10m"
  },
  {
    "name": "K8S HPA At Max Replicas",
    "is_paused": false,
    "severity": "warning",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-019-hpa-at-max-replicas",
    "expression": "kube_horizontalpodautoscaler_status_current_replicas >= kube_horizontalpodautoscaler_spec_max_replicas",
    "threshold": 0,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\\n*HPA:* {{ with $values.A }}{{ .Labels.horizontalpodautoscaler }}{{ end }}\\n\\nWARNING: HPA has scaled to maximum replicas. Consider increasing max replicas or optimizing application.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nHPA is no longer at maximum replicas."
    },
    "for": "15m"
  },
  {
    "name": "K8S High Network IO",
    "is_paused": false,
    "severity": "warning",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-020-high-network-io",
    "expression": "sum by (instance) (rate(node_network_receive_bytes_total{device!~\"lo|veth.*|docker.*|flannel.*|cali.*|cbr.*\"}[5m]) + rate(node_network_transmit_bytes_total{device!~\"lo|veth.*|docker.*|flannel.*|cali.*|cbr.*\"}[5m])) > 1000000000",
    "threshold": 1000000000,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Node:* {{ with $values.A }}{{ .Labels.instance }}{{ end }}\\n\\nWARNING: Node network I/O > 1 GB/s. Potential bandwidth saturation.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nNode network I/O has returned to normal levels."
    },
    "for": "10m"
  },
  {
    "name": "K8S Pod Restart Count High",
    "is_paused": false,
    "severity": "warning",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-021-pod-restart-count-high",
    "expression": "increase(kube_pod_container_status_restarts_total[1h]) > 5",
    "threshold": 5,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\\n*Pod:* {{ with $values.A }}{{ .Labels.pod }}{{ end }}\\n*Container:* {{ with $values.A }}{{ .Labels.container }}{{ end }}\\n\\nWARNING: Pod has restarted {{ with $values.A }}{{ .Value }}{{ end }} times in the last hour. Review logs for crash causes.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nPod restart rate has returned to normal."
    },
    "for": "0m"
  },
  {
    "name": "K8S Pod CrashLoopBackOff",
    "is_paused": false,
    "severity": "critical",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-001-pod-not-running",
    "expression": "sum by (namespace, pod, container) (kube_pod_container_status_waiting_reason{reason=\"CrashLoopBackOff\"}) > 0",
    "threshold": 0,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\\n*Pod:* {{ with $values.A }}{{ .Labels.pod }}{{ end }}\\n*Container:* {{ with $values.A }}{{ .Labels.container }}{{ end }}\\n\\nCRITICAL: Container is in CrashLoopBackOff. Check logs: `kubectl logs {{ with $values.A }}{{ .Labels.pod }}{{ end }} -c {{ with $values.A }}{{ .Labels.container }}{{ end }} -n {{ with $values.A }}{{ .Labels.namespace }}{{ end }}`",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nContainer is no longer in CrashLoopBackOff."
    },
    "for": "2m"
  },
  {
    "name": "K8S Pod ImagePullBackOff",
    "is_paused": false,
    "severity": "critical",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-001-pod-not-running",
    "expression": "sum by (namespace, pod, container) (kube_pod_container_status_waiting_reason{reason=~\"ImagePullBackOff|ErrImagePull\"}) > 0",
    "threshold": 0,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\\n*Pod:* {{ with $values.A }}{{ .Labels.pod }}{{ end }}\\n*Container:* {{ with $values.A }}{{ .Labels.container }}{{ end }}\\n\\nCRITICAL: Container image pull failed. Check image name, tag, and pull secrets.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nContainer image has been successfully pulled."
    },
    "for": "2m"
  },
  {
    "name": "K8S Node Not Ready",
    "is_paused": false,
    "severity": "critical",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-006-node-cpu-critical",
    "expression": "kube_node_status_condition{condition=\"Ready\", status=\"true\"} == 0",
    "threshold": 0,
    "exec_err_state": "OK",
    "condition": "eq",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Node:* {{ with $values.A }}{{ .Labels.node }}{{ end }}\\n\\nCRITICAL: Node is not in Ready state. Check kubelet status and node connectivity.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nNode is now Ready."
    },
    "for": "2m"
  },
  {
    "name": "K8S Node Memory Pressure",
    "is_paused": false,
    "severity": "warning",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-007-node-memory-critical",
    "expression": "kube_node_status_condition{condition=\"MemoryPressure\", status=\"true\"} == 1",
    "threshold": 1,
    "exec_err_state": "OK",
    "condition": "eq",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Node:* {{ with $values.A }}{{ .Labels.node }}{{ end }}\\n\\nWARNING: Node is under memory pressure. Pods may be evicted.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nNode memory pressure has been resolved."
    },
    "for": "2m"
  },
  {
    "name": "K8S Node Disk Pressure",
    "is_paused": false,
    "severity": "warning",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-008-node-disk-space-critical",
    "expression": "kube_node_status_condition{condition=\"DiskPressure\", status=\"true\"} == 1",
    "threshold": 1,
    "exec_err_state": "OK",
    "condition": "eq",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Node:* {{ with $values.A }}{{ .Labels.node }}{{ end }}\\n\\nWARNING: Node is under disk pressure. Pods may be evicted. Clean up disk space.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nNode disk pressure has been resolved."
    },
    "for": "2m"
  },
  {
    "name": "K8S Node PID Pressure",
    "is_paused": false,
    "severity": "warning",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-006-node-cpu-critical",
    "expression": "kube_node_status_condition{condition=\"PIDPressure\", status=\"true\"} == 1",
    "threshold": 1,
    "exec_err_state": "OK",
    "condition": "eq",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Node:* {{ with $values.A }}{{ .Labels.node }}{{ end }}\\n\\nWARNING: Node is under PID pressure. Too many processes running.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nNode PID pressure has been resolved."
    },
    "for": "2m"
  },
  {
    "name": "K8S PVC Pending",
    "is_paused": false,
    "severity": "warning",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-003-statefulset-replica-mismatch",
    "expression": "kube_persistentvolumeclaim_status_phase{phase=\"Pending\"} == 1",
    "threshold": 1,
    "exec_err_state": "OK",
    "condition": "eq",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\\n*PVC:* {{ with $values.A }}{{ .Labels.persistentvolumeclaim }}{{ end }}\\n\\nWARNING: PersistentVolumeClaim is pending. Check storage class and available PVs.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nPersistentVolumeClaim is now bound."
    },
    "for": "5m"
  },
  {
    "name": "K8S PV Available But Not Bound",
    "is_paused": false,
    "severity": "info",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-003-statefulset-replica-mismatch",
    "expression": "kube_persistentvolume_status_phase{phase=\"Available\"} == 1",
    "threshold": 1,
    "exec_err_state": "OK",
    "condition": "eq",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\\n*PV:* {{ with $values.A }}{{ .Labels.persistentvolume }}{{ end }}\\n\\nINFO: PersistentVolume is available but not bound. This may be expected for dynamically provisioned PVs.",
      "ResolvedAlertValues": "*Component:* Kubernetes\\n\\nPersistentVolume is now bound."
    },
    "for": "30m"
  },
  {
    "name": "K8S Pod OOMKilled",
    "is_paused": false,
    "severity": "warning",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-010-pod-memory-approaching-limit",
    "expression": "sum by (namespace, pod, container) (kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}) > 0 and sum by (namespace, pod, container) (changes(kube_pod_container_status_restarts_total[10m])) > 0",
    "threshold": 0,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Kubernetes\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\n*Pod:* {{ with $values.A }}{{ .Labels.pod }}{{ end }}\n*Container:* {{ with $values.A }}{{ .Labels.container }}{{ end }}\n\nWARNING: Container was OOMKilled recently. This alert triggered because a restart was detected in the last 10 minutes with an OOM reason.",
      "ResolvedAlertValues": "*Component:* Kubernetes\n\nNo recent OOMKill events detected in the last window."
    },
    "for": "5m"
  },
  {
    "name": "Pods High CPU Usage",
    "is_paused": false,
    "severity": "warning",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Kubernetes_Alerting_Runbook.md#k8s-010-pod-memory-approaching-limit",
    "expression": "100 * (\n  sum by (pod, namespace) (rate(container_cpu_usage_seconds_total{container!=\"\", pod!~\"(collector|gmp-operator|node-local-dns|anetd|netd).*\"}[5m])) \n  / \n  sum by (pod, namespace) (kube_pod_container_resource_requests{resource=\"cpu\", pod!~\"(collector|gmp-operator|node-local-dns|anetd|netd).*\"})\n)",
    "threshold": 80,
    "exec_err_state": "OK",
    "no_data_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "{{ with $values.A }}*Namespace:* {{ .Labels.namespace }}\n*Pod:* {{ .Labels.pod }}\n\nWARNING: CPU usage at {{ printf \"%.1f\" .Value }}% of requests (> 80%). Consider scaling or query optimization.{{ end }}",
      "ResolvedAlertValues": "{{ with $values.A }}*Namespace:* {{ .Labels.namespace }}\n*Pod:* {{ .Labels.pod }}\n\nCPU usage recovered to {{ printf \"%.1f\" .Value }}%.{{ end }}"
    },
    "for": "10m"
  },
  {
    "name": "Pods High Memory Usage",
    "is_paused": false,
    "severity": "warning",
    "doc": "https://github.com/sarvamai/grafana-dashboards/tree/main/sarvam-os/signoz/CloudNativePG_Alerting_Runbook.md#cnpg-021-high-memory-utilization",
    "expression": "100 * (\n  sum by (pod, namespace) (container_memory_working_set_bytes{container!=\"\", pod!~\"(anetd|gmp-operator|collector|node-local-dns-chkrz|netd).*\"}) \n  / \n  sum by (pod, namespace) (kube_pod_container_resource_requests{resource=\"memory\", pod!~\"(anetd|gmp-operator|collector|node-local-dns-chkrz|anetd|netd).*\"})\n) > 90",
    "threshold": 90,
    "exec_err_state": "OK",
    "no_data_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "{{ with $values.A }}*Namespace:* {{ .Labels.namespace }}\n*Pod:* {{ .Labels.pod }}\n\nWARNING: Memory usage at {{ printf \"%.1f\" .Value }}% of requests (> 90%). OOM risk.{{ end }}",
      "ResolvedAlertValues": "{{ with $values.A }}*Namespace:* {{ .Labels.namespace }}\n*Pod:* {{ .Labels.pod }}\n\nMemory usage recovered to {{ printf \"%.1f\" .Value }}%.{{ end }}"
    },
    "for": "5m"
  },
  {
    "name": "Infra Pod CPU Critical",
    "is_paused": false,
    "severity": "critical",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Infra_Alerting_Runbook.md#infra-001-pod-cpu-critical",
    "expression": "100 * sum by (namespace, pod) (rate(container_cpu_usage_seconds_total{namespace=~\"postgres|clickhouse-samvaad|kong|kafka|monitoring|redis\", container!=\"\"}[5m])) / sum by (namespace, pod) (kube_pod_container_resource_limits{namespace=~\"postgres|clickhouse-samvaad|kong|kafka|monitoring|redis\", resource=\"cpu\"})",
    "threshold": 90,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Infrastructure\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\n*Pod:* {{ with $values.A }}{{ .Labels.pod }}{{ end }}\n\nCRITICAL: Pod CPU usage is at {{ printf \"%.0f\" .Value }}% of its limit. Performance degradation likely.",
      "ResolvedAlertValues": "*Component:* Infrastructure\n\nPod CPU usage has returned to normal levels."
    },
    "for": "5m"
  },
  {
    "name": "Infra Pod Memory Critical",
    "is_paused": false,
    "severity": "critical",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Infra_Alerting_Runbook.md#infra-002-pod-memory-critical",
    "expression": "100 * sum by (namespace, pod) (container_memory_working_set_bytes{namespace=~\"postgres|clickhouse-samvaad|kong|kafka|monitoring|redis\", container!=\"\"}) / sum by (namespace, pod) (kube_pod_container_resource_limits{namespace=~\"postgres|clickhouse-samvaad|kong|kafka|monitoring|redis\", resource=\"memory\"})",
    "threshold": 90,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Infrastructure\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\n*Pod:* {{ with $values.A }}{{ .Labels.pod }}{{ end }}\n\nCRITICAL: Pod Memory usage is at {{ printf \"%.0f\" .Value }}% of its limit. Risk of OOMKill.",
      "ResolvedAlertValues": "*Component:* Infrastructure\n\nPod Memory usage has returned to normal levels."
    },
    "for": "5m"
  },
  {
    "name": "Infra Pod CPU Warning",
    "is_paused": false,
    "severity": "warning",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Infra_Alerting_Runbook.md#infra-003-pod-cpu-warning",
    "expression": "100 * sum by (namespace, pod) (rate(container_cpu_usage_seconds_total{namespace=~\"postgres|clickhouse-samvaad|kong|kafka|monitoring|redis\", container!=\"\"}[5m])) / sum by (namespace, pod) (kube_pod_container_resource_limits{namespace=~\"postgres|clickhouse-samvaad|kong|kafka|monitoring|redis\", resource=\"cpu\"})",
    "threshold": 80,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Infrastructure\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\n*Pod:* {{ with $values.A }}{{ .Labels.pod }}{{ end }}\n\nWARNING: Pod CPU usage is high ({{ printf \"%.0f\" .Value }}%).",
      "ResolvedAlertValues": "*Component:* Infrastructure\n\nPod CPU usage has returned to normal levels."
    },
    "for": "10m"
  },
  {
    "name": "Infra Pod Memory Warning",
    "is_paused": false,
    "severity": "warning",
    "doc": "https://github.com/sarvamai/grafana-dashboards/blob/main/sarvam-os/signoz/Infra_Alerting_Runbook.md#infra-004-pod-memory-warning",
    "expression": "100 * sum by (namespace, pod) (container_memory_working_set_bytes{namespace=~\"postgres|clickhouse-samvaad|kong|kafka|monitoring|redis\", container!=\"\"}) / sum by (namespace, pod) (kube_pod_container_resource_limits{namespace=~\"postgres|clickhouse-samvaad|kong|kafka|monitoring|redis\", resource=\"memory\"})",
    "threshold": 80,
    "exec_err_state": "OK",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "*Component:* Infrastructure\n*Namespace:* {{ with $values.A }}{{ .Labels.namespace }}{{ end }}\n*Pod:* {{ with $values.A }}{{ .Labels.pod }}{{ end }}\n\nWARNING: Pod Memory usage is high ({{ printf \"%.0f\" .Value }}%).",
      "ResolvedAlertValues": "*Component:* Infrastructure\n\nPod Memory usage has returned to normal levels."
    },
    "for": "10m"
  },
  {
    "name": "High ClickHouse Connection Count",
    "is_paused": false,
    "expression": "chi_clickhouse_metric_TCPConnection + chi_clickhouse_metric_HTTPConnection + chi_clickhouse_metric_InterserverConnection",
    "threshold": 20,
    "exec_err_state": "KeepLast",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "{{ with $values.A }}\n  [Pod: {{ .Labels.pod }}] [CHI: {{ .Labels.chi }}] [env: production] High number of ClickHouse connections detected. (Actual: {{ .Value }} > Threshold: 10).\n{{ end }}",
      "ResolvedAlertValues": "{{ with $values.A }}\n  [Pod: {{ .Labels.pod }}] [CHI: {{ .Labels.chi }}] [env: production] ClickHouse connection count back to normal. (Actual: {{ .Value }} < Threshold: 10).\n{{ end }}"
    },
    "for": "30s"
  },
  {
    "name": "ClickHouse RWLock Writers Waiting",
    "is_paused": false,
    "expression": "chi_clickhouse_metric_RWLockWaitingWriters",
    "threshold": 0,
    "exec_err_state": "KeepLast",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "{{ with $values.A }}\n  [Pod: {{ .Labels.pod }}] [CHI: {{ .Labels.chi }}] [env: production] ClickHouse writers are waiting on RWLock. Contention detected. ({{ .Value }} writers waiting > 0).\n{{ end }}",
      "ResolvedAlertValues": "{{ with $values.A }}\n  [Pod: {{ .Labels.pod }}] [CHI: {{ .Labels.chi }}] [env: production] ClickHouse RWLock contention resolved. ({{ .Value }} writers waiting).\n{{ end }}"
    },
    "for": "1m"
  },
  {
    "name": "ClickHouse High Disk Usage",
    "is_paused": false,
    "expression": "100 * (1 - (chi_clickhouse_metric_DiskFreeBytes / chi_clickhouse_metric_DiskTotalBytes))",
    "threshold": 85,
    "exec_err_state": "KeepLast",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "{{ with $values.A }}\n  [Disk: {{ .Labels.disk }}] [Pod: {{ .Labels.pod }}] [CHI: {{ .Labels.chi }}] [env: production] ClickHouse disk usage is high: {{ .Value }}% used (Threshold: 85%).\n{{ end }}",
      "ResolvedAlertValues": "{{ with $values.A }}\n  [Disk: {{ .Labels.disk }}] [Pod: {{ .Labels.pod }}] [CHI: {{ .Labels.chi }}] [env: production] ClickHouse disk usage back to normal: {{ .Value }}% used.\n{{ end }}"
    },
    "for": "1m"
  },
  {
    "name": "ClickHouse High Memory Usage",
    "is_paused": false,
    "expression": "100 * (1 - (chi_clickhouse_metric_OSMemoryFreePlusCached / chi_clickhouse_metric_OSMemoryTotal))",
    "threshold": 85,
    "exec_err_state": "KeepLast",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "{{ with $values.A }}\n  [Pod: {{ .Labels.pod }}] [CHI: {{ .Labels.chi }}] [env: production] High node memory usage: {{ .Value }}% used (Threshold: 85%).\n{{ end }}",
      "ResolvedAlertValues": "{{ with $values.A }}\n  [Pod: {{ .Labels.pod }}] [CHI: {{ .Labels.chi }}] [env: production] Node memory usage normalized: {{ .Value }}%.\n{{ end }}"
    },
    "for": "1m"
  },
  {
    "name": "ClickHouse CPU Overload",
    "is_paused": true,
    "expression": "chi_clickhouse_metric_OSCPUOverload",
    "threshold": 0,
    "exec_err_state": "KeepLast",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "{{ with $values.A }}\n  [Pod: {{ .Labels.pod }}] [CHI: {{ .Labels.chi }}] [env: production] ClickHouse CPU overload detected. Overloaded cores: {{ .Value }} > 0.\n{{ end }}",
      "ResolvedAlertValues": "{{ with $values.A }}\n  [Pod: {{ .Labels.pod }}] [CHI: {{ .Labels.chi }}] [env: production] CPU overload resolved. Overloaded cores: {{ .Value }}.\n{{ end }}"
    },
    "for": "1m"
  },
  {
    "name": "ClickHouse High Query Preemptions",
    "is_paused": false,
    "expression": "rate(chi_clickhouse_metric_QueryPreempted[1m])",
    "threshold": 10,
    "exec_err_state": "KeepLast",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "{{ with $values.A }}\n  [Pod: {{ .Labels.pod }}] [CHI: {{ .Labels.chi }}] [env: production] High rate of query preemptions: {{ .Value }}/s > 10/s. Indicates query concurrency/thread contention.\n{{ end }}",
      "ResolvedAlertValues": "{{ with $values.A }}\n  [Pod: {{ .Labels.pod }}] [CHI: {{ .Labels.chi }}] [env: production] Query preemption rate back to normal: {{ .Value }}/s.\n{{ end }}"
    },
    "for": "2m"
  },
  {
    "name": "ClickHouse High Query Threads",
    "is_paused": false,
    "expression": "chi_clickhouse_metric_QueryThread",
    "threshold": 100,
    "exec_err_state": "KeepLast",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "{{ with $values.A }}\n  [Pod: {{ .Labels.pod }}] [CHI: {{ .Labels.chi }}] [env: production] Too many concurrent query threads: {{ .Value }} > 100.\n{{ end }}",
      "ResolvedAlertValues": "{{ with $values.A }}\n Query thread count normalized: [env: production] {{ .Value }}.\n{{ end }}"
    },
    "for": "1m"
  },
  {
    "name": "ClickHouse QueryPreempted Priority Wait",
    "is_paused": false,
    "expression": "avg_over_time(rate(chi_clickhouse_metric_QueryPreempted[1m])[10m:])",
    "threshold": 0,
    "exec_err_state": "KeepLast",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "{{ with $values.A }}\n  [Pod: {{ .Labels.pod }}] [CHI: {{ .Labels.chi }}] [env: production] Queries are being preempted due to 'priority' settings. ({{ .Value }} preemptions/sec average over 10 minutes > 0).\n{{ end }}",
      "ResolvedAlertValues": "{{ with $values.A }}\n  [Pod: {{ .Labels.pod }}] [env: production] Query preemptions due to 'priority' settings resolved. ({{ .Value }} average/sec).\n{{ end }}"
    },
    "for": "1m"
  },
  {
    "name": "ClickHouse Merge Overload",
    "is_paused": false,
    "expression": "avg_over_time(chi_clickhouse_metric_Merge[1h])",
    "threshold": 15,
    "exec_err_state": "KeepLast",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "{{ with $values.A }}\n  [Pod: {{ .Labels.pod }}] [CHI: {{ .Labels.chi }}] [env: production] Background merges are saturating the merge pool. (Avg: {{ .Value }} > Threshold: 15 over 1h).\n{{ end }}",
      "ResolvedAlertValues": "{{ with $values.A }}\n  [Pod: {{ .Labels.pod }}] Merge activity normalized. [env: production] (Avg: {{ .Value }}).\n{{ end }}"
    },
    "for": "2m"
  },
  {
    "name": "ClickHouse DistributedFiles Pending Warning",
    "is_paused": false,
    "expression": "chi_clickhouse_metric_DistributedFilesToInsert",
    "threshold": 50,
    "exec_err_state": "KeepLast",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "{{ with $values.A }}\n  [CHI: {{ .Labels.chi }}] [env: production] Warning: {{ .Value }} distributed insert files pending > 50.\n{{ end }}",
      "ResolvedAlertValues": "{{ with $values.A }}\n Distributed insert queue normalized: [env: production] {{ .Value }} pending.\n{{ end }}"
    },
    "for": "2m"
  },
  {
    "name": "ClickHouse Query Load High Warning",
    "is_paused": false,
    "expression": "chi_clickhouse_metric_Query",
    "threshold": 70,
    "exec_err_state": "KeepLast",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "{{ with $values.A }}\n  [CHI: {{ .Labels.chi }}] [Pod: {{ .Labels.pod }}] [env: production] High query concurrency warning: {{ .Value }} executing queries > 70 (70% of max).\n{{ end }}",
      "ResolvedAlertValues": "{{ with $values.A }}\n  Query concurrency warning resolved. [env: production] Now: {{ .Value }}.\n{{ end }}"
    },
    "for": "2m"
  },
  {
    "name": "ClickHouseDelayedInserts",
    "is_paused": false,
    "expression": "rate(chi_clickhouse_metric_DelayedInserts[1m])",
    "threshold": 0,
    "exec_err_state": "KeepLast",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "{{ with $values.A }}\n  [Pod: {{ .Labels.pod }}] [CHI: {{ .Labels.chi }}] [env: production] INSERTs are being delayed or rejected due to high number of active parts in MergeTree. Current rate: {{ .Value }}/s.\n{{ end }}",
      "ResolvedAlertValues": "{{ with $values.A }}\n  INSERT delays resolved.[env: production] Delay rate back to normal: {{ .Value }}/s.\n{{ end }}"
    },
    "for": "2m"
  },
  {
    "name": "ClickHouse Replication DelayHigh",
    "is_paused": false,
    "expression": "chi_clickhouse_metric_ReplicasMaxAbsoluteDelay",
    "threshold": 300,
    "exec_err_state": "KeepLast",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "{{ with $values.A }}\n  [CHI: {{ .Labels.chi }}] [Pod: {{ .Labels.pod }}] [env: production] Replication lag is high: {{ .Value }} seconds > 300. Replica is falling behind.\n{{ end }}",
      "ResolvedAlertValues": "{{ with $values.A }}\n  Replication delay normalized: [env: production] {{ .Value }} seconds.\n{{ end }}"
    },
    "for": "2m"
  },
  {
    "name": "High CPU Usage per Pod in ClickHouse (vs Requests)",
    "is_paused": false,
    "expression": "100 * (sum by(pod, namespace) (rate(container_cpu_usage_seconds_total{container!=\"\", namespace=\"clickhouse\"}[1m])) / sum by(pod, namespace) (kube_pod_container_resource_requests{resource=\"cpu\", unit=\"core\", namespace=\"clickhouse\"}))",
    "threshold": 75,
    "exec_err_state": "Ok",
    "no_data_state": "Ok",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "{{ with $values.A }}\n[Team: platform] [env: production]  \nCPU usage: `{{ printf \"%.2f\" .Value }}`% (> 75%) on pod `{{ .Labels.pod }}`.\n{{ end }}",
      "ResolvedAlertValues": "{{ with $values.A }}\n[Team: platform] [env: production]  \nCPU usage recovered: `{{ printf \"%.2f\" .Value }}`% (≤ 75%) on pod `{{ .Labels.pod }}`.\n{{ end }}"
    },
    "for": "2m"
  },
  {
    "name": "High Memory Usage per Pod in ClickHouse (vs Requests)",
    "is_paused": false,
    "expression": "100 * (sum by(pod, namespace) (container_memory_usage_bytes{container!=\"\", namespace=\"clickhouse\"}) / sum by(pod, namespace) (kube_pod_container_resource_requests{resource=\"memory\", unit=\"byte\", namespace=\"clickhouse\"}))",
    "threshold": 75,
    "exec_err_state": "Ok",
    "no_data_state": "Ok",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "{{ with $values.A }}\n[Team: platform] [env: production]  \nMemory usage: `{{ printf \"%.2f\" .Value }}`% (> 75%) on pod `{{ .Labels.pod }}`.\n{{ end }}",
      "ResolvedAlertValues": "{{ with $values.A }}\n[Team: platform] [env: production]  \nMemory usage recovered: `{{ printf \"%.2f\" .Value }}`% (≤ 75%) on pod `{{ .Labels.pod }}`.\n{{ end }}"
    },
    "for": "2m"
  },
  {
    "name": "ClickHouse PVC Disk Usage (All PVCs)",
    "is_paused": false,
    "expression": "100 * (kubelet_volume_stats_used_bytes{namespace=\"clickhouse\"} / kubelet_volume_stats_capacity_bytes{namespace=\"clickhouse\"})",
    "threshold": 70,
    "exec_err_state": "KeepLast",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "{{ with $values.A }}\n  [PVC: {{ .Labels.persistentvolumeclaim }}] [Pod: {{ .Labels.pod }}] [Team: infra] [env: production] High disk usage on PVC `{{ .Labels.persistentvolumeclaim }}` (Usage: {{ .Value }}% > Threshold: 85%).\n{{ end }}",
      "ResolvedAlertValues": "{{ with $values.A }}\n  [PVC: {{ .Labels.persistentvolumeclaim }}] [Pod: {{ .Labels.pod }}] [Team: infra] [env: production] PVC disk usage is back to normal for `{{ .Labels.persistentvolumeclaim }}` (Usage: {{ .Value }}% < Threshold: 85%).\n{{ end }}"
    },
    "for": "2m"
  },
  {
    "name": "ClickHouse Disk Data Size High",
    "is_paused": false,
    "expression": "chi_clickhouse_metric_DiskDataBytes{chi=~\".*\", hostname=~\".*\"} / (1024 * 1024 * 1024)",
    "threshold": 350,
    "exec_err_state": "KeepLast",
    "condition": "gt",
    "annotations": {
      "FiringAlertValues": "{{ with $values.A }}\n  [Hostname: {{ .Labels.hostname }}] [CHI: {{ .Labels.chi }}] [Team: infra] [env: production] ClickHouse disk usage high ({{ .Value }} GB > 400 GB).\n{{ end }}",
      "ResolvedAlertValues": "{{ with $values.A }}\n  [Hostname: {{ .Labels.hostname }}] [CHI: {{ .Labels.chi }}] [Team: infra] [env: production] ClickHouse disk usage back to normal ({{ .Value }} GB < 400 GB).\n{{ end }}"
    },
    "for": "5m"
  }
]